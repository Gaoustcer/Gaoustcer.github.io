---
layout:     post
title:      PyTorch教程——张量操作与线性回归
subtitle:   PyTorch
date:       2021-04-16
author:     Haihan Gao
header-img: img/post-bg-swift2.jpg
catalog: true
tags:
    - Machine Learning
    - PyTorch
---
# PyTorch教程——张量操作与线性回归

## 张量的操作

### 拼接

#### torch.cat()

```python
torch.cat(tensors,dim=0,out=None) #tensor是一个张量组成的列表，按照dim维度进行拼接
```

#### torch.stack()

```python
torch.stack(tensors,dim=0,out=None) #将张量在新创建的dim维度上进行拼接
```

### 切分

#### torch.chunk()

```python
torch.chunk(input,chunks,dim=0) #张量按照dim进行平均切分，返回张量组成的列表
```

#### torch.split()

```python
torch.split(tensor,split_size,dim=0) #split_size是一个列表，表示每一维的切割大小
```

### 索引

#### torch.index_select()

```python
torch.index_select(input,dim,index,out=None) #在维度dim上，按照index索引取出数据并拼接为张量返回
```

注意，index应该是tensor类型，而且元素类型应该为int

####  torch.mask_select()

```python
torch.masked_select(input, mask, out=None) #按照mask中的true进行True索引拼接得到的一维张量返回，mask是一个与input同形状的bool类型张量
```

### 变换

#### torch.reshape()

```python
torch.reshape(input,shape) #变换张量的形状，而且返回的张量与原来的张量共享数据内存
```

shape是一个元组，若元组的某一个维度值是-1，那么这个维度的大小由其它元组决定

#### torch.transpose()

```python
torch.transpose(input,dim0,dim1)
```

交换张量的两个维度，矩阵转置

#### torch.t()

二维张量转置

#### torch.squeeze()

```python
torch.squeeze(input,dim=None,out=None)
```

* dim若为None，则移除所有长度为1的维度，若指定维度，则当且仅当该长度为1是可以移除

## 张量的数学运算

### torch.add()

```python
torch.add(input,other,out=None)
torch.add(input,other,*,alpha=1,out=None) #逐元素计算input+alpha*other
```

### torch.addcdiv/addcmul

```python
torch.addcdiv(input,tensor1,tensor2,*,value=1,out=None)
```

$$out_i=input_i+value\times \frac{tensor1_i}{tensor2_i}$$

## 线性回归

* 确定模型 $y=\omega x+b$
* 选择损失函数，均方误差
* 梯度下降求解梯度，选择适当的学习率
  * $\omega=\omega-lr\times \omega_{grad}$
  * $b=b-lr\times b_{grad}$

### autograd方法

* 定义z是一个标量，z=x**2+y，z.backward()自动计算出叶子节点的梯度值
* 如果z是一个向量或是矩阵，需要定义grad_tensor计算矩阵的梯度

```python
torch.autograd.backward(
	tensors, #可以作为调用方法的对象
	grad_tensors=None, #计算矩阵梯度用到，其实也是一个tensor而且shape和前面的tensor保持一致
	retain_graph=None,
	create_graph=False,#设置为true计算更高阶的精度
	grad_variables=None)
```

训练的时候注意，每次更新参数之后都需要更新向量的维度，而且向量相减需要调用sub方法