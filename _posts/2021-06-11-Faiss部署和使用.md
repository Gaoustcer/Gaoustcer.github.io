---
layout:     post
title:      Faiss部署和使用
subtitle:   Faiss
date:       2021-06-11
author:     Haihan Gao
header-img: img/post-bg-swift2.jpg
catalog: true
tags:
    - Faiss
    - Search model
---
# Faiss部署和使用

**这篇文档记录了我部署和尝试使用Faiss的一些过程，操作系统是Windows10，处理器硬件平台是i5-1035G1**

## 背景介绍

文本匹配问题中，我们通常会将文本抽象为向量，然后根据向量之间的相似度，比如余弦值或是欧几里得距离判断Query和document之间的相似度，但是，当文本数量很多的时候，遍历所有向量计算相似度显然是不现实的，为此我们介绍Faiss这个工程上加速这一过程的工具。

Faiss全称是Facebook AI Similarity Search，针对大规模相似度检索问题开发的一个工具，对于10亿量级的索引可以做到毫秒级别的检索性能。

Faiss的基本原理是将自己的候选向量封装为一个index数据库，可以加速我们检索相似向量TopK的过程，并且支持GPU构建。

假设我们已经有了n个向量，$x_1,x_2,...,x_n$,现在加入一个向量x，最邻近搜索问题是找到$i=argmin_i||x-x_i||$,Faiss可以

* 找到k个最邻近向量
* 一个返回一组向量
* 在speed,memory,precision之间做tradeoff
* 使用最大化内积而不是欧几里得距离最小衡量向量相似度

## 构建Faiss环境

网上有教程使用conda安装Faiss包，输入以下命令

```shell
conda install -c pytorch faiss-cpu
```

但是我没有配置成功，`import faiss`报错找不到模块，查了GitHub上面有人提过这个issue，貌似是包源的问题，因为有的包来自conda-forge，但是这个来自pytorch，所以需要创建一个虚拟环境

```shell
conda create -n faiss_env -c pytorch python=3.8 faiss-cpu
conda activate faiss_env
```

然后是第二个坑，我在vscode里激活了这个虚拟环境，但是跑出来如下报错

![image-20210512171731072](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20210512171731072.png)

放弃vscode，使用PyCharm建立工程，选择生成的虚拟环境作为解释器环境，导入成功

## Faiss原理

向量计算是一个时空优化问题，增加索引可以提升查询速度，但是却有存储空间开销。为了得到时间和空间综合性能最优，我们使用PCA和PQ两个手段进行向量压缩和编码

### PCA降维

目的是降低数据集的维度，移动/旋转坐标轴，找到方差最大方向上的特征值，基本步骤是：

* 计算数据集的协方差矩阵(m维的数据协方差矩阵为$m\times m$大小)
* 计算协方差矩阵的特征值和特征向量
* 保留最重要的n个特征向量，将m维数据投影到n维空间

### PQ编码

* 乘积量化PQ
* 将高维向量分割成多个低维向量，并对每一组低维向量聚类，用中心点代替这个向量

## Faiss上手

* 得到向量库，假设xb是数据库向量库，xq是查询向量库
* 用faiss构建index，并将向量添加到index中
* 用faiss index检索

### 创建index

```python
index = faiss.IndexFlatL2(d)
```

Faiss围绕index构建，index封装了数据库向量的集合，并且对数据做了预处理使之有效，上述`IndexFlatL2`方法创建了一个L2距离的index

所有index需要直到他们构建在多少维度的向量上，d提供了这个参数，大部分index需要通过training得知向量分布，但是这里`IndexFlatL2`不需要

### 向index中添加向量集合

```python
index.add(xb)      
```

`index.nototal`表明了这个index中含有多少向量，`index.is_trained`表明index是否需要被训练

### Searching

最基本的搜索是k-近邻搜索，对于一个大小为nq的检索进行k-近邻搜索得到的结果存储于一个$nq\times k$大小的整型矩阵中，第i行表明第i个查询向量最邻近的k个向量编号，search方法还会返回一个$nq\times k$的浮点数矩阵，表明的是相似度

## 衡量搜索引擎的性能指标

### MAP

#### Precision(P)

准确度反映的是检索得到的文档中相关文档占的比例

#### Recall(R)

召回率反映的是在全部文档集合中被检索到的文档比例

#### Average Precision(AveP)

$$AveP=\frac{\sum_{k=1}^n(P(k)\times rel(k))}{total\ relevant}$$

P(k)是前k个文档的准确率，rel(k)表示第k个文档是否相关，相关为1否则为0

#### Mean average precision(MAP)

对于多个查询请求，比如Q个query，用MAP衡量检索系统的性能

$$MAP=\frac{\sum_{q=1}^Q AveP(q)}{Q}$$

### nDCG

#### CG

表示前p个位置累计得到的收益，公式为

$CG_p=\sum_{i=1}^p rel_i$ $rel_i$表现文档的相关度等级 

#### DCG

解决CG对位置信息不敏感的问题，第i个位置的价值为$\frac{1}{\log_2(i+1)}$

$DCG_p=\sum_{i=1}^p \frac{rel_i}{\log_2(i+1)}$或者$DCG_p=\sum_{i=1}^p \frac{2^{rel_i}-1}{\log_2(i+1)}$

#### Normalize DCG

查询语句得到的文本集合长度不一致，p值不同会对DCG计算产生较大的影响，不能对不同的DCG求平均，需要进行归一化，先引入IDCG的概念

$IDCG_p=\sum_{i=1}^{|REL|}\frac{2^{rel_i}-1}{\log_2(i+1)}$ |REL|表示文档按照相关性从大到小排序，取前p个文档组成的集合

$nDCG_p=\frac{DCG_p}{IDCG_p}$

## Faiss重要类简介

### index

是一个基类，后续各种类均继承自这个类，含有如下成员对象

* d，每个向量的维度
* ntotal,索引的向量的数目
* metric_type检索时使用的metric类型，衡量距离的标准

### indexFlat

用于暴力搜索，无需train，添加所有的向量然后query的时候逐一比对即可

### Clustering

实现K-means聚类的类，提供train接口，需要训练数据和index，得到训练数据每一类的类中心向量，包括以下过程

* search过程，聚类中心加入index，并对训练数据做search
* 计算新的聚类中心